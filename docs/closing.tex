\chapter{Conclusion}
In conclusion, a handful of learning machines were used to perform, and the feasibility of the implementation solutions and corresponding learning machines has been clearly illustrated. However, this came with a lot of challenges, such as the computational resources needed to use the implemented pipeline and the experiment interface. This is in part due to the inefficiencies linked to the programming language used, namely \textit{Python} and its limitation.\newline
As a solution to these inefficiencies the use of some more bare-bone programming languages, such as \textit{rust} and \textit{C++}, can be highly advised due to the lack of garbage collection, which makes the experimentation with the implemented experiment module with normal pc-hardware prohibitive. During the optimization process, the direct use of the garbage collection mechanism in \textit{python} led to a screeching slowdown of the parsing and the extraction of xml-files, as this triggered a complete and thorough analysis of the memory usage instead of driving out the parsed xml-tree, which takes a big chunk of memory due to the size of each xml-file and the number of elements in them. The solution used proved to be a good patchwork and allowed for the speed-up of the prototyping and the experimenting, but didn't solve the problem in its entirety a consequence of the size of the matrixfied motions themselves.\newline
Nevertheless, the foundations of the practical part of this thesis are solid and can be expanded or used for other purposes, which is a consequence of the modular approach used. In other words, I can say that the design process of this project can be incorporated into other problems with minimal tweaking and some parts can serve for other projects. To that end, the experimentation process allows for the use of high-performance computing to contrast the results of using multiple methods and automation the process of visualization. This allows data scientists better insight into the general picture, as demonstrated by the evaluation chapter of this thesis.\newline
As for the learning machine part, the results of this comparative study are a pretty underwhelming failure. As the models implement showed very little promise and the accuracy rates over all trials and experiments were on the negative side. This can be in part due to the misappropriation of the KIT dataset, as the method used to convert the annotations into labels has no basis in early research and is based on primitive and rudimentary techniques and tools. As such one can't definitely gauge the role played by the use of these methods in these results. However, one can not understate the complexity of the problem at hand and must take this analysis with a great deal of skepticism as extracting features from time series of information about the position of the joints and the position and rotation of the pelvis is an arduous and computationally expensive endeavor. Consequently, the models implemented in this thesis showed little results and had very spotty accuracy and erratic predictions.\newline
A solution might be the search for more meaningful information, which would probably allow for the extraction of better features, thus leading to better and more accurate learning machines. As outlined in \ref{subsec:preliminary_analysis} one could store more information in the xml-files, such as the velocity and the acceleration rate of the rotation of each joint, and even expand in the notation by adding more child elements to the \textit{MotionFrame} element. This makes the mmm-notation a good foundation for the persistent storage of motion data. Concordantly, this expansion can be achieved by using a physics engine and feeding the information available in the current xml-files to it to generate more context, such as the torque at each joint, the velocity of some body parts, etc... However, this is a very complex process involving the creation of an urdf-model that is very realistic and physically sound.\newline
Sadly, these urdf-models were first implemented as prototyping tools for very simple robots and are very limited with regard to the intricacies of the human body and the physics that must be modeled to achieve the realism level needed. This can even spark the implementation of new prototyping tools for embedded systems that use HAR to monitor and log human activities, as these systems are very limited in the scope of accessible information and can't impede the normal flow of the daily life of the user. This would be the case if these systems need information about all the body parts to perform the intended task. An example of that would be using a smartwatch to track the velocity acceleration and position of the hand and classify the activities performed accordingly. To this end, researchers can use this proposed simulation to generate only the information about the hands and train models based on it, thus eliminating the need for costly recording sessions and the hiring of entire teams to record motions for the purpose of data gathering. In addition to that, using data gathered with devices in real life is highly and strictly regulated and is linked to privacy issues, thus making the process of sample gathering for the purpose of training learning machines more ineffective and costly. Lastly, this will lead to the overcoming of some inherent problems in HAR, as discussed in \ref{subsec:probHAR}, such as class imbalance and annotation scarcity.
